{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b65d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Set, Optional\n",
    "\n",
    "@dataclass\n",
    "class DataSet:\n",
    "    features:    List[float] = field(default_factory=list)\n",
    "    labels:      List[float] = field(default_factory=list)\n",
    "    ex_ret:     List[float] = field(default_factory=list) \n",
    "    rows: int = 0\n",
    "    cols: int = 0\n",
    "\n",
    "@dataclass\n",
    "class DataPack:\n",
    "    train: DataSet = field(default_factory=DataSet)  # 60 %\n",
    "    val:   DataSet = field(default_factory=DataSet)  # 20 % – threshold search\n",
    "    test:  DataSet = field(default_factory=DataSet)  # 20 % – final verification\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    id:               int   = 0\n",
    "    max_depth:        int   = 5\n",
    "    eta:              float = 0.05\n",
    "    num_round:        int   = 400\n",
    "    objective:        str   = \"binary:logistic\"\n",
    "    eval_metric:      str   = \"auc\"\n",
    "    min_child_weight: float = 1.0\n",
    "    scale_pos_weight: float = 1.0\n",
    "    subsample:        float = 1.0\n",
    "    colsample:        float = 1.0\n",
    "\n",
    "@dataclass\n",
    "class ValidationMetrics:\n",
    "    sharpe_ratio:          float = 0.0\n",
    "    avg_return:            float = 0.0\n",
    "    avg_pd:                float = 0.0\n",
    "    approved_count:        int   = 0\n",
    "    approved_rate:         float = 0.0\n",
    "    best_pd_threshold:     float = 0.0\n",
    "    best_return_threshold: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    best_cls_config: ModelConfig       = field(default_factory=ModelConfig)\n",
    "    best_reg_config: ModelConfig       = field(default_factory=ModelConfig)\n",
    "    best_metrics:    ValidationMetrics = field(default_factory=ValidationMetrics)\n",
    "\n",
    "@dataclass\n",
    "class ExperimentContext:\n",
    "    pred_pd:         List[float] = field(default_factory=list)\n",
    "    pred_est_return: List[float] = field(default_factory=list)\n",
    "    ex_returns:      List[float] = field(default_factory=list)\n",
    "    bond_yields:     List[float] = field(default_factory=list)\n",
    "    test_size:       int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvLoader:\n",
    "\n",
    "    def __init__(self, filename: str, target_col_name: str, ignore_col_names: Set[str]):\n",
    "        self._fname       = filename\n",
    "        self._target_col  = target_col_name\n",
    "        self._ignore_col  = set(ignore_col_names)\n",
    "        self._feature_names: List[str] = []\n",
    "\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        return self._feature_names\n",
    "\n",
    "    def _read_df(self):\n",
    "        print(\"[CsvLoader] Configuring columns...\")\n",
    "        df = pd.read_csv(self._fname)\n",
    "\n",
    "        if self._target_col not in df.columns:\n",
    "            raise RuntimeError(f\"Target column not found: {self._target_col}\")\n",
    "\n",
    "        feature_cols = [\n",
    "            c for c in df.columns\n",
    "            if c != self._target_col and c not in self._ignore_col\n",
    "        ]\n",
    "        self._feature_names = feature_cols\n",
    "        return df, feature_cols\n",
    "\n",
    "    def _df_to_dataset(self, df: pd.DataFrame, feature_cols: List[str]) -> DataSet:\n",
    "        ds             = DataSet()\n",
    "        ds.rows        = len(df)\n",
    "        ds.cols        = len(feature_cols)\n",
    "        ds.features    = df[feature_cols].astype(\"float32\").values.flatten().tolist()\n",
    "        ds.labels      = df[self._target_col].astype(\"float32\").tolist()\n",
    "        ds.ex_ret     = df[\"Excess_Return\"].astype(\"float32\").tolist() if \"Excess_Return\" in df.columns else [0.0] * ds.rows\n",
    "        return ds\n",
    "   \n",
    "    def load(self) -> DataSet:\n",
    "        df, feature_cols = self._read_df()\n",
    "        print(f\"[CsvLoader] Loaded {len(df)} rows, {len(feature_cols)} feature cols.\")\n",
    "        return self._df_to_dataset(df, feature_cols)\n",
    "\n",
    "    def load_and_split(self, train_ratio: float = 0.6, val_ratio: float = 0.2) -> DataPack:\n",
    "        print(\">>> [CsvLoader] Loading raw data...\")\n",
    "        df, feature_cols = self._read_df()\n",
    "\n",
    "        print(f\">>> [CsvLoader] Shuffling and Splitting \"\n",
    "              f\"(Train: {train_ratio}, Val: {val_ratio})...\")\n",
    "\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        n         = len(df)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end   = train_end + int(n * val_ratio)\n",
    "\n",
    "        pack = DataPack(\n",
    "            train = self._df_to_dataset(df.iloc[:train_end],        feature_cols),\n",
    "            val   = self._df_to_dataset(df.iloc[train_end:val_end], feature_cols),\n",
    "            test  = self._df_to_dataset(df.iloc[val_end:],          feature_cols),\n",
    "        )\n",
    "\n",
    "        print(\">>> Split Complete.\")\n",
    "        print(f\" - Train Rows: {pack.train.rows}\")\n",
    "        print(f\" - Val Rows:   {pack.val.rows}\")\n",
    "        print(f\" - Test Rows:  {pack.test.rows}\")\n",
    "        return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ab4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentManager:\n",
    "    candidate_depths: List[int]   = [5, 7]\n",
    "    candidate_etas:   List[float] = [0.05, 0.01]\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dmatrix(dataset: DataSet, use_returns: bool = False) -> xgb.DMatrix:\n",
    "        X = np.array(dataset.features, dtype=np.float32).reshape(dataset.rows, dataset.cols)\n",
    "        y = np.array(dataset.returns if use_returns else dataset.labels, dtype=np.float32)\n",
    "        return xgb.DMatrix(X, label=y, missing=float(\"nan\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def _train_booster(dm: xgb.DMatrix, config: ModelConfig) -> xgb.Booster:\n",
    "        params = {\n",
    "            \"verbosity\":        0,\n",
    "            \"tree_method\":      \"hist\",\n",
    "            \"objective\":        config.objective,\n",
    "            \"eval_metric\":      config.eval_metric,\n",
    "            \"max_depth\":        config.max_depth,\n",
    "            \"eta\":              config.eta,\n",
    "            \"min_child_weight\": config.min_child_weight,\n",
    "            \"scale_pos_weight\": config.scale_pos_weight,\n",
    "            \"subsample\":        config.subsample,\n",
    "            \"colsample_bytree\": config.colsample,\n",
    "            \"nthread\":          os.cpu_count() // 4,\n",
    "        }\n",
    "        return xgb.train(params, dm, num_boost_round=config.num_round, verbose_eval=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_grid(classification: bool) -> List[ModelConfig]:\n",
    "        obj    = \"binary:logistic\"   if classification else \"reg:absoluteerror\"\n",
    "        metric = \"auc\"               if classification else \"mae\"\n",
    "\n",
    "        configs: List[ModelConfig] = []\n",
    "        id_counter = 1\n",
    "        for depth in ExperimentManager.candidate_depths:\n",
    "            for eta in ExperimentManager.candidate_etas:\n",
    "                rounds = int(20.0 / eta)\n",
    "                rounds = max(100, min(3000, rounds))\n",
    "                configs.append(ModelConfig(\n",
    "                    id=id_counter, max_depth=depth, eta=eta, num_round=rounds,\n",
    "                    objective=obj, eval_metric=metric,\n",
    "                    min_child_weight=1.0, scale_pos_weight=1.0,\n",
    "                    subsample=0.8, colsample=0.8,\n",
    "                ))\n",
    "                id_counter += 1\n",
    "        return configs\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_sharpe_ratio(ex_rets:List[float], approvals:List[bool]) -> float:\n",
    "        n = len(ex_rets)\n",
    "        ex_rets_arr = np.array(ex_rets)\n",
    "        approvals_arr = np.array(approvals)\n",
    "\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "\n",
    "        excess = np.where(approvals_arr, ex_rets_arr, 0.0)\n",
    "        mean_excess = np.mean(excess)\n",
    "        std_dev = np.std(excess, ddof=1)\n",
    "        return 0.0 if std_dev < 1e-9 else float(mean_excess / std_dev)\n",
    "\n",
    "    def _prepare_experiment(self,\n",
    "                            dataset:     DataSet,\n",
    "                            cls_config:  ModelConfig,\n",
    "                            reg_config:  ModelConfig,\n",
    "                            split_ratio: float) -> ExperimentContext:\n",
    "        total_rows  = dataset.rows\n",
    "        split_point = int(total_rows * split_ratio)\n",
    "        test_size   = total_rows - split_point\n",
    "\n",
    "        if test_size <= 0:\n",
    "            print(\"Error: Test set size is 0.\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "\n",
    "        nc = dataset.cols\n",
    "\n",
    "        train_ds = DataSet(\n",
    "            features    = dataset.features[: split_point * nc],\n",
    "            labels      = dataset.labels[: split_point],\n",
    "            returns     = dataset.ex_ret[: split_point],\n",
    "            rows=split_point, cols=nc,\n",
    "        )\n",
    "        test_ds = DataSet(\n",
    "            features    = dataset.features[split_point * nc :],\n",
    "            labels      = dataset.labels[split_point:],\n",
    "            returns     = dataset.ex_ret[split_point:],\n",
    "            rows=test_size, cols=nc,\n",
    "        )\n",
    "\n",
    "        print(\">>> [Common] Training Best Models...\")\n",
    "        booster_cls = self._train_booster(self._make_dmatrix(train_ds, False), cls_config)\n",
    "        booster_reg = self._train_booster(self._make_dmatrix(train_ds, True),  reg_config)\n",
    "\n",
    "        X_test  = np.array(test_ds.features, dtype=np.float32).reshape(test_size, nc)\n",
    "        dm_test = xgb.DMatrix(X_test, missing=float(\"nan\"))\n",
    "\n",
    "        print(f\">>> [Common] Predicting Test Set ({test_size} rows)...\")\n",
    "        return ExperimentContext(\n",
    "            pred_pd         = booster_cls.predict(dm_test).tolist(),\n",
    "            pred_est_return = booster_reg.predict(dm_test).tolist(),\n",
    "            ex_returns  = test_ds.returns[:],\n",
    "            bond_yields     = test_ds.bond_yields[:],\n",
    "            test_size       = test_size,\n",
    "        )\n",
    "\n",
    "    def _find_best_thresholds(self, ctx: ExperimentContext) -> ValidationMetrics:\n",
    "        best = ValidationMetrics(sharpe_ratio=-999.0)\n",
    "\n",
    "        pd_candidates  = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "        ret_candidates = [0.02, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
    "\n",
    "        n        = ctx.test_size\n",
    "        approval = [False] * n\n",
    "\n",
    "        for pd_th in pd_candidates:\n",
    "            for ret_th in ret_candidates:\n",
    "                approved_count = 0\n",
    "                sum_ret = sum_pd = 0.0\n",
    "\n",
    "                for i in range(n):\n",
    "                    b_pass = (ctx.pred_pd[i] < pd_th) and (ctx.pred_est_return[i] > ret_th)\n",
    "                    approval[i] = b_pass\n",
    "                    if b_pass:\n",
    "                        approved_count += 1\n",
    "                        sum_ret += ctx.ex_returns[i]\n",
    "                        sum_pd  += ctx.pred_pd[i]\n",
    "\n",
    "                if approved_count < 10:\n",
    "                    continue\n",
    "\n",
    "                sharpe = self._calculate_sharpe_ratio(ctx.ex_returns, ctx.bond_yields, approval)\n",
    "                if sharpe > best.sharpe_ratio:\n",
    "                    best.sharpe_ratio          = sharpe\n",
    "                    best.best_pd_threshold     = pd_th\n",
    "                    best.best_return_threshold = ret_th\n",
    "                    best.approved_count        = approved_count\n",
    "                    best.approved_rate         = approved_count / n * 100.0\n",
    "                    best.avg_return            = sum_ret / approved_count\n",
    "                    best.avg_pd                = sum_pd  / approved_count\n",
    "\n",
    "        return best\n",
    "\n",
    "    def _train_model_on_set(self, train_set: DataSet, config: ModelConfig) -> xgb.Booster:\n",
    "        use_returns = \"binary\" not in config.objective\n",
    "        return self._train_booster(self._make_dmatrix(train_set, use_returns), config)\n",
    "\n",
    "    def _predict_on_set(self,\n",
    "                        booster_cls: xgb.Booster,\n",
    "                        booster_reg: xgb.Booster,\n",
    "                        target_set:  DataSet) -> ExperimentContext:\n",
    "        X  = np.array(target_set.features, dtype=np.float32).reshape(target_set.rows, target_set.cols)\n",
    "        dm = xgb.DMatrix(X, missing=float(\"nan\"))\n",
    "        return ExperimentContext(\n",
    "            pred_pd         = booster_cls.predict(dm).tolist(),\n",
    "            pred_est_return = booster_reg.predict(dm).tolist(),\n",
    "            ex_returns  = target_set.ex_ret[:],\n",
    "            test_size       = target_set.rows,\n",
    "        )\n",
    "\n",
    "    def run_grid_search_auto(self, pack: DataPack) -> ExperimentResult:\n",
    "        cls_configs = self._generate_grid(True)\n",
    "        reg_configs = self._generate_grid(False)\n",
    "        total_iter  = len(cls_configs) * len(reg_configs)\n",
    "\n",
    "        print(\"\\n>>> Automatic Grid Search on Validation Set\")\n",
    "        print(\">>> Training on 60% (Train), Evaluating on 20% (Val)\")\n",
    "        print(f\">>> Total Combinations: {total_iter}\")\n",
    "\n",
    "        best_result = ExperimentResult()\n",
    "        best_result.best_metrics.sharpe_ratio = -999.0\n",
    "        cur_round = 0\n",
    "\n",
    "        with open(\"grid_search_auto.csv\", \"w\") as f:\n",
    "            f.write(\"Iter,Cls_Depth,Cls_Eta,Reg_Depth,Reg_Eta,\"\n",
    "                    \"Best_PD_Thresh,Best_Ret_Thresh,\"\n",
    "                    \"Approved_Cnt,Avg_Return,Avg_PD,Sharpe_Ratio\\n\")\n",
    "\n",
    "            for c_conf in cls_configs:\n",
    "                for r_conf in reg_configs:\n",
    "                    cur_round += 1\n",
    "\n",
    "                    h_cls   = self._train_model_on_set(pack.train, c_conf)\n",
    "                    h_reg   = self._train_model_on_set(pack.train, r_conf)\n",
    "                    val_ctx = self._predict_on_set(h_cls, h_reg, pack.val)\n",
    "                    metrics = self._find_best_thresholds(val_ctx)\n",
    "\n",
    "                    marker = \"\"\n",
    "                    if metrics.sharpe_ratio > best_result.best_metrics.sharpe_ratio:\n",
    "                        best_result.best_metrics    = metrics\n",
    "                        best_result.best_cls_config = c_conf\n",
    "                        best_result.best_reg_config = r_conf\n",
    "                        marker = \" [★ BEST]\"\n",
    "\n",
    "                    print(f\"[{cur_round}/{total_iter}] \"\n",
    "                          f\"C(d{c_conf.max_depth} e{c_conf.eta:.2f}) \"\n",
    "                          f\"R(d{r_conf.max_depth} e{r_conf.eta:.2f}) \"\n",
    "                          f\"| Val-Sharpe: {metrics.sharpe_ratio:.5f}{marker}\")\n",
    "\n",
    "                    f.write(f\"{cur_round},{c_conf.max_depth},{c_conf.eta},\"\n",
    "                            f\"{r_conf.max_depth},{r_conf.eta},\"\n",
    "                            f\"{metrics.best_pd_threshold},{metrics.best_return_threshold},\"\n",
    "                            f\"{metrics.approved_count},{metrics.avg_return},\"\n",
    "                            f\"{metrics.avg_pd},{metrics.sharpe_ratio}\\n\")\n",
    "                    f.flush()\n",
    "\n",
    "        print(\"\\n>>> Grid Search Complete.\")\n",
    "        print(f\">>> Best Validation Sharpe: {best_result.best_metrics.sharpe_ratio}\")\n",
    "        return best_result\n",
    "\n",
    "    def run_standard_validation(self,\n",
    "                                pack:     DataPack,\n",
    "                                best_cls: ModelConfig,\n",
    "                                best_reg: ModelConfig) -> None:\n",
    "       \n",
    "        print(f\">>> Training Models on Train Set ({pack.train.rows} rows)...\")\n",
    "        h_cls = self._train_model_on_set(pack.train, best_cls)\n",
    "        h_reg = self._train_model_on_set(pack.train, best_reg)\n",
    "\n",
    "        print(f\">>> Optimizing Thresholds on Validation Set ({pack.val.rows} rows)...\")\n",
    "        val_ctx      = self._predict_on_set(h_cls, h_reg, pack.val)\n",
    "        best_metrics = self._find_best_thresholds(val_ctx)\n",
    "\n",
    "        print(f\"   -> Found Best Thresholds on Validation Set:\")\n",
    "        print(f\"      PD < {best_metrics.best_pd_threshold},  \"\n",
    "              f\"Return > {best_metrics.best_return_threshold}\")\n",
    "        print(f\"      Val Sharpe Ratio: {best_metrics.sharpe_ratio}\")\n",
    "\n",
    "        print(f\">>>Final Verification on Test Set ({pack.test.rows} rows)...\")\n",
    "        test_ctx = self._predict_on_set(h_cls, h_reg, pack.test)\n",
    "\n",
    "        final_approval: List[bool] = []\n",
    "        approved_count = 0\n",
    "        for i in range(test_ctx.test_size):\n",
    "            b_pass = (\n",
    "                test_ctx.pred_pd[i]         < best_metrics.best_pd_threshold and\n",
    "                test_ctx.pred_est_return[i] > best_metrics.best_return_threshold\n",
    "            )\n",
    "            final_approval.append(b_pass)\n",
    "            if b_pass:\n",
    "                approved_count += 1\n",
    "\n",
    "        if approved_count >= 10:\n",
    "            final_sharpe = self._calculate_sharpe_ratio(\n",
    "                test_ctx.ex_returns, test_ctx.bond_yields, final_approval\n",
    "            )\n",
    "        else:\n",
    "            print(f\">>> [WARNING] Not enough approved samples ({approved_count} < 10). \"\n",
    "                  f\"Sharpe set to 0.0.\")\n",
    "            final_sharpe = 0.0\n",
    "\n",
    "        print(\"\\n------------------------------------------------------\")\n",
    "        print(\">>> [Final Result]\")\n",
    "        print(f\"1. Validation Sharpe : {best_metrics.sharpe_ratio}\")\n",
    "        print(f\"2. Test Set Sharpe   : {final_sharpe}\")\n",
    "        print(f\"3. Approved Count    : {approved_count} / {test_ctx.test_size} \"\n",
    "              f\"({approved_count / test_ctx.test_size * 100:.1f}%)\")\n",
    "\n",
    "        if abs(final_sharpe - best_metrics.sharpe_ratio) < 0.5:\n",
    "            print(\">>> SUCCESS: Model is Robust! (Val & Test scores are similar)\")\n",
    "        else:\n",
    "            print(\">>> WARNING: Large gap detected. Check for Overfitting.\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "        self._perform_random_permutation_test(\n",
    "            test_ctx.pred_pd, test_ctx.pred_est_return,\n",
    "            test_ctx.ex_returns, test_ctx.bond_yields,\n",
    "            best_metrics.best_pd_threshold,\n",
    "            best_metrics.best_return_threshold,\n",
    "            1000\n",
    "        )\n",
    "\n",
    "    def _perform_random_permutation_test(self,\n",
    "                                         pred_pd:     List[float],\n",
    "                                         pred_ret:    List[float],\n",
    "                                         actual_ret:  List[float],\n",
    "                                         bond_yields: List[float],\n",
    "                                         best_pd_th:  float,\n",
    "                                         best_ret_th: float,\n",
    "                                         iterations:  int) -> None:\n",
    "        print(\"\\n======================================================\")\n",
    "        print(\">>> [Permutation Test] Verifying Best Strategy Significance <<<\")\n",
    "        print(\"======================================================\")\n",
    "\n",
    "        test_size = len(actual_ret)\n",
    "        if test_size == 0:\n",
    "            return\n",
    "\n",
    "        fixed_approval = [\n",
    "            (pred_pd[i] < best_pd_th) and (pred_ret[i] > best_ret_th)\n",
    "            for i in range(test_size)\n",
    "        ]\n",
    "        approved_count  = sum(fixed_approval)\n",
    "        original_sharpe = self._calculate_sharpe_ratio(actual_ret, bond_yields, fixed_approval)\n",
    "\n",
    "        print(f\"Best Config: PD < {best_pd_th}, Ret > {best_ret_th}\")\n",
    "        print(f\"Original Sharpe Ratio: {original_sharpe} (Count: {approved_count})\")\n",
    "        print(f\"Running {iterations} permutations...\")\n",
    "\n",
    "        better_count   = 0\n",
    "        indices        = list(range(test_size))\n",
    "        rng            = random.Random()\n",
    "        shuffled_ret   = [0.0] * test_size\n",
    "        shuffled_bonds = [0.0] * test_size\n",
    "        progress_step  = max(1, iterations // 10)\n",
    "\n",
    "        for k in range(iterations):\n",
    "            rng.shuffle(indices)\n",
    "            for i in range(test_size):\n",
    "                shuffled_ret[i]   = actual_ret[indices[i]]\n",
    "                shuffled_bonds[i] = bond_yields[indices[i]]\n",
    "\n",
    "            random_sharpe = self._calculate_sharpe_ratio(shuffled_ret, shuffled_bonds, fixed_approval)\n",
    "            if random_sharpe >= original_sharpe:\n",
    "                better_count += 1\n",
    "\n",
    "            if (k + 1) % progress_step == 0:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # p-value = (betterCount + 1) / (iterations + 1)\n",
    "        p_value = (better_count + 1) / (iterations + 1)\n",
    "\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"Permutation Test Result:\")\n",
    "        print(f\" - Better Random Strategies: {better_count} / {iterations}\")\n",
    "        print(f\" - P-Value: {p_value:.4f}\")\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(\">>> SUCCESS: The strategy is Statistically Significant! (Not Luck)\")\n",
    "        else:\n",
    "            print(\">>> WARNING: The strategy might be due to randomness.\")\n",
    "        print(\"======================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file      = \"Data1.csv\"\n",
    "target_column = \"\"\n",
    "ignore_list: Set[str] = {\n",
    "        \"Actual_term\", \"total_pymnt\", \"last_pymnt_amnt\", \"내부수익률\",\n",
    "        \"loan_status\", \"Return\",\n",
    "}\n",
    "\n",
    "print(\">>> [1/4] Data Loading...\")\n",
    "loader    = CsvLoader(csv_file, target_column, ignore_list)\n",
    "data_pack = loader.load_and_split(0.6, 0.2)\n",
    "\n",
    "if data_pack.test.rows == 0:\n",
    "    print(\"Error: No data found.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "total_rows = data_pack.train.rows + data_pack.val.rows + data_pack.test.rows\n",
    "print(f\" -> Load Complete: {total_rows} Rows, {data_pack.test.cols} Cols\")\n",
    "\n",
    "manager = ExperimentManager()\n",
    "print(\"\\n>>> [1/4] Find BestConfig from Robust Range...\")\n",
    "print(f\"    - ETA Range:   {ExperimentManager.candidate_etas}\")\n",
    "print(f\"    - Depth Range: {ExperimentManager.candidate_depths}\")\n",
    "\n",
    "result = manager.run_grid_search_auto(data_pack)\n",
    "\n",
    "print(\"\\n>>> [2/4] Starting Integrated Grid Search...\")\n",
    "print(\"    - Strategy: Dual Model (Classification + Regression)\")\n",
    "print(\"    - Optimization: Parameter Grid + Threshold Auto-Tuning\")\n",
    "\n",
    "manager.run_standard_validation(data_pack, result.best_cls_config, result.best_reg_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snu_team07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
